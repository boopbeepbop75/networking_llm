{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "from data_loading.tools import reduce_mem_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Base URL of the dataset\n",
    "BASE_URL = \"http://malnet.cc.gatech.edu/image-data/\"\n",
    "SAVE_DIR = \"data/MalNet_Dataset\"\n",
    "\n",
    "def ensure_directory(path):\n",
    "    \"\"\"Ensure the directory exists.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def get_links(url):\n",
    "    \"\"\"Get all links (subdirectories and files) from a given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to access {url}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    links = []\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        href = link.get(\"href\")\n",
    "        if href and href not in (\"../\", \"/\"):  # Ignore parent directory links\n",
    "            full_url = urljoin(url, href)\n",
    "            links.append(full_url)\n",
    "    return links\n",
    "\n",
    "def download_file(url, save_path):\n",
    "    \"\"\"Download a file with a progress bar.\"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get(\"content-length\", 0))\n",
    "    with open(save_path, \"wb\") as file, tqdm(\n",
    "        desc=os.path.basename(save_path),\n",
    "        total=total_size,\n",
    "        unit=\"B\",\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for data in response.iter_content(chunk_size=1024):\n",
    "            file.write(data)\n",
    "            bar.update(len(data))\n",
    "\n",
    "def crawl_and_download(url, save_path):\n",
    "    \"\"\"Recursively crawl and download all files from a directory URL.\"\"\"\n",
    "    ensure_directory(save_path)\n",
    "    links = get_links(url)\n",
    "    \n",
    "    for link in links:\n",
    "        parsed = urlparse(link)\n",
    "        if parsed.path.endswith(\"/\") and \"6GB\" not in parsed.path:  # If it's a directory, recurse. Exclude the 6GB directory\n",
    "            subdir_name = os.path.basename(os.path.normpath(parsed.path))\n",
    "            crawl_and_download(link, os.path.join(save_path, subdir_name))\n",
    "        else:  # Otherwise, it's a file\n",
    "            filename = os.path.basename(parsed.path)\n",
    "            file_path = os.path.join(save_path, filename)\n",
    "            if not os.path.exists(file_path):  # Avoid re-downloading\n",
    "                download_file(link, file_path)\n",
    "\n",
    "        time.sleep(1) # Be polite\n",
    "\n",
    "# Start crawling and downloading\n",
    "crawl_and_download(BASE_URL, SAVE_DIR)\n",
    "\n",
    "print(\"Download complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
